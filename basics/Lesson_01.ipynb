{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=fnDYXE7BFto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "import dotenv\n",
    "\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(input_files=[\"./datasets/lora_paper.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 1\n",
      "file_name: lora_paper.pdf\n",
      "file_path: datasets\\lora_paper.pdf\n",
      "file_type: application/pdf\n",
      "file_size: 1609513\n",
      "creation_date: 2024-06-09\n",
      "last_modified_date: 2024-06-09\n",
      "\n",
      "LORA: L OW-RANK ADAPTATION OF LARGE LAN-\n",
      "GUAGE MODELS\n",
      "Edward Hu‚àóYelong Shen‚àóPhillip Wallis Zeyuan Allen-Zhu\n",
      "Yuanzhi Li Shean Wang Lu Wang Weizhu Chen\n",
      "Microsoft Corporation\n",
      "{edwardhu, yeshe, phwallis, zeyuana,\n",
      "yuanzhil, swang, luw, wzchen }@microsoft.com\n",
      "yuanzhil@andrew.cmu.edu\n",
      "(Version 2)\n",
      "ABSTRACT\n",
      "An important paradigm of natural language processing consists of large-scale pre-\n",
      "training on general domain data and adaptation to particular tasks or domains. As\n",
      "we pre-train larger models, full Ô¨Åne-tuning, which retrains all model parameters,\n",
      "becomes less feasible. Using GPT-3 175B as an example ‚Äì deploying indepen-\n",
      "dent instances of Ô¨Åne-tuned models, each with 175B parameters, is prohibitively\n",
      "expensive. We propose Low-RankAdaptation, or LoRA, which freezes the pre-\n",
      "trained model weights and injects trainable rank decomposition matrices into each\n",
      "layer of the Transformer architecture, greatly reducing the number of trainable pa-\n",
      "rameters for downstream tasks. Compared to GPT-3 175B Ô¨Åne-tuned with Adam,\n",
      "LoRA can reduce the number of trainable parameters by 10,000 times and the\n",
      "GPU memory requirement by 3 times. LoRA performs on-par or better than Ô¨Åne-\n",
      "tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite hav-\n",
      "ing fewer trainable parameters, a higher training throughput, and, unlike adapters,\n",
      "no additional inference latency . We also provide an empirical investigation into\n",
      "rank-deÔ¨Åciency in language model adaptation, which sheds light on the efÔ¨Åcacy of\n",
      "LoRA. We release a package that facilitates the integration of LoRA with PyTorch\n",
      "models and provide our implementations and model checkpoints for RoBERTa,\n",
      "DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA .\n",
      "1 I NTRODUCTION\n",
      "Pretrained \n",
      "Weights\n",
      "ùëä‚àà‚Ñùùëë√óùëë\n",
      "xh\n",
      "ùêµ=0\n",
      "ùê¥=ùí©(0,ùúé2)\n",
      "ùëëùëüPretrained \n",
      "Weights\n",
      "ùëä‚àà‚Ñùùëë√óùëë\n",
      "xf(x)\n",
      "ùëë\n",
      "Figure 1: Our reparametriza-\n",
      "tion. We only train AandB.Many applications in natural language processing rely on adapt-\n",
      "ingonelarge-scale, pre-trained language model to multiple down-\n",
      "stream applications. Such adaptation is usually done via Ô¨Åne-tuning ,\n",
      "which updates all the parameters of the pre-trained model. The ma-\n",
      "jor downside of Ô¨Åne-tuning is that the new model contains as many\n",
      "parameters as in the original model. As larger models are trained\n",
      "every few months, this changes from a mere ‚Äúinconvenience‚Äù for\n",
      "GPT-2 (Radford et al., b) or RoBERTa large (Liu et al., 2019) to a\n",
      "critical deployment challenge for GPT-3 (Brown et al., 2020) with\n",
      "175 billion trainable parameters.1\n",
      "Many sought to mitigate this by adapting only some parameters or\n",
      "learning external modules for new tasks. This way, we only need\n",
      "to store and load a small number of task-speciÔ¨Åc parameters in ad-\n",
      "dition to the pre-trained model for each task, greatly boosting the\n",
      "operational efÔ¨Åciency when deployed. However, existing techniques\n",
      "‚àóEqual contribution.\n",
      "0Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency.\n",
      "1While GPT-3 175B achieves non-trivial performance with few-shot learning, Ô¨Åne-tuning boosts its perfor-\n",
      "mance signiÔ¨Åcantly as shown in Appendix A.\n",
      "1arXiv:2106.09685v2  [cs.CL]  16 Oct 2021\n"
     ]
    }
   ],
   "source": [
    "note_metadata = nodes[0].get_content(metadata_mode=True)\n",
    "print(str(note_metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create LLM and Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.llm = OpenAI(model='gpt-3.5-turbo')\n",
    "Settings.embedding = OpenAIEmbedding(model='text-embedding-ada-002')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "\n",
    "summary_index = SummaryIndex(nodes=nodes)\n",
    "vecto_index = VectorStoreIndex(nodes=nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Querys Engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_model=\"tree_summary\",\n",
    "    use_async=True,\n",
    ")\n",
    "\n",
    "vector_query_engine = vecto_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        \"Useful for summarizing of the lora paper.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "vecto_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=(\n",
    "        \"Useful for retrieving specific context related to the lora paper.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Router Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "\n",
    "query_egine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[summary_tool, vecto_tool],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 0: The first choice is more relevant as it focuses on summarizing the lora paper, which would provide an overview of the subject matter..\n",
      "\u001b[0mO assunto do paper \"LoRA\" √© a proposta de uma estrat√©gia eficiente de adapta√ß√£o para modelos de linguagem, que n√£o introduz lat√™ncia de infer√™ncia e mant√©m a qualidade do modelo. A abordagem LoRA permite a troca r√°pida de tarefas quando implantada como um servi√ßo, compartilhando a maioria dos par√¢metros do modelo. A pesquisa se concentra em modelos de linguagem Transformer, mas os princ√≠pios propostos s√£o aplic√°veis a qualquer rede neural com camadas densas.\n"
     ]
    }
   ],
   "source": [
    "response = query_egine.query(\"Sobre qual √© o assunto do lora paper?\")\n",
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
